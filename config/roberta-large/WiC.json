{
    "pretrained_weight":  "roberta-large",
    "dataset": "fewglue/WiC",
    "max_text_length": 256,
    "batch_size": 1,
    "eval_batch_size": 8,
    "num_batches": 1000,
    "eval_every": 250,
    "warmup_ratio": 0.06,
    "grad_accumulation_factor": 16,
    "max_num_lbl_tok": 1,
    "seed": 42,
    "lr": 1e-5,
    "dropout_rate": 0.1,
    "weight_decay": 1e-2,
    "pattern": 1,
    "eval_train": true
}
